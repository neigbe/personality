{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import tiktoken\n",
    "import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "from data_processing import pers_labels\n",
    "from openai import OpenAI\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=getpass.getpass(\"please enter your openai api key\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "PWD = os.environ[\"WORKSPACE_PATH\"]\n",
    "\n",
    "data_type = \"big 5_4_all_lbls\"\n",
    "\n",
    "label_mode = pers_labels.MBPT if pers_labels.MBPT.lower() in data_type else pers_labels.BIG_5\n",
    "\n",
    "index = [idx for idx in range(5) if str(idx) in data_type][0]\n",
    "\n",
    "pers_defs = {\n",
    "    pers_labels.MBPT: {\n",
    "        0: ((\"I\", \"introverted\"), (\"E\", \"extroverted\")),\n",
    "        1: ((\"S\", \"sensing\"), (\"N\", \"intuitive\")),\n",
    "        2: ((\"F\", \"feeling\"), (\"T\", \"thinking\")),\n",
    "        3: ((\"J\", \"judging\"), (\"P\", \"perceiving\")),\n",
    "    },\n",
    "    pers_labels.BIG_5: {\n",
    "        0: ((\"S\", \"social\"), (\"R\", \"reserved\")),\n",
    "        1: ((\"L\", \"limbic\"), (\"C\", \"calm\")),\n",
    "        2: ((\"O\", \"organized\"), (\"U\", \"unstructured\")),\n",
    "        3: ((\"A\", \"agreeable\"), (\"E\", \"egocentric\")),\n",
    "        4: ((\"N\", \"non-curious\"), (\"I\", \"inquisitive\")),\n",
    "    }\n",
    "}\n",
    "\n",
    "((label1, label1_def), (label2, label2_def)) = pers_defs[label_mode][index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{PWD}/data/cornell_movies/speakers.json\", \"r+\") as fp:\n",
    "    fp_parsed = json.load(fp)\n",
    "    chars_meta = {}\n",
    "    chars_meta_rows = []\n",
    "    for char in fp_parsed:\n",
    "        meta = fp_parsed[char][\"meta\"]\n",
    "        meta[\"character_name\"] = meta[\"character_name\"].lower()\n",
    "        meta[\"char_id\"] = char\n",
    "        chars_meta[char] = meta\n",
    "        chars_meta_rows.append(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_json(f\"{PWD}/data/datasets/{data_type}.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, char, num_votes):\n",
    "\n",
    "    _, scene = text.split(\"\\n\", 1)\n",
    "\n",
    "    # To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Read the scenes below and then categorize {char}'s personality as {label1} for {label1_def} or \"{label2}\" for {label2_def}, according to the {label_mode.lower()} personality typology. Response with only one word.\n",
    "\n",
    "scenes:\n",
    "    {scene}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = enc.decode(enc.encode(prompt)[:4000])\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  # GPT-3.5 Turbo engine\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=1,\n",
    "        n=num_votes,\n",
    "    )\n",
    "\n",
    "    # Extract the first choice (response)\n",
    "    predicted_label = [choice.message.content for choice in response.choices]\n",
    "    return prompt, predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = [char_id for char_id in chars_meta]\n",
    "preds = defaultdict(list)\n",
    "labels = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 866/866 [08:45<00:00,  1.65it/s]\n"
     ]
    }
   ],
   "source": [
    "for char_id in tqdm.tqdm(dataset.char_id.unique()):\n",
    "    if char_id in preds:\n",
    "        continue\n",
    "    char_name = chars_meta[char_id][\"character_name\"]\n",
    "\n",
    "    char_rows = dataset[dataset.char_id == char_id]\n",
    "\n",
    "    char_labels = char_rows.label.tolist()\n",
    "\n",
    "    scene = dataset.text[0]\n",
    "\n",
    "    prompt, response = classify_text(scene, char_name, min(100, len(char_labels)))\n",
    "\n",
    "    votes = [r.upper() for r in response if r.upper() in [label1, label2]]\n",
    "\n",
    "    # print(prompt)\n",
    "    # print()\n",
    "\n",
    "    # print(\"Correct:\", row.label)\n",
    "    # print(\"Predicted:\", response)\n",
    "    preds[char_id] = votes\n",
    "    labels[char_id] = char_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{PWD}/data/gpt_preds/{data_type}.json\", \"w+\") as fp:\n",
    "    json.dump(preds, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overall metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_len = len(preds)\n",
    "# preds_len = 1897\n",
    "\n",
    "with open(f\"{PWD}/data/gpt_preds/{data_type}.json\", \"r+\") as fp:\n",
    "    old_preds = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "preds = {}\n",
    "\n",
    "for char_id in chars:\n",
    "    if len(old_preds.get(char_id, [])) > 0:\n",
    "        labels[char_id] = dataset[dataset.char_id == char_id].label.tolist()\n",
    "        preds[char_id] = old_preds[char_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/label_encoders/big 5_4_all_lbls.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/naomi/pers_proj/cs229/code/Querying GPT (Simulation).ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B20.69.237.131/home/naomi/pers_proj/cs229/code/Querying%20GPT%20%28Simulation%29.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../data/label_encoders/\u001b[39;49m\u001b[39m{\u001b[39;49;00mdata_type\u001b[39m}\u001b[39;49;00m\u001b[39m.pkl\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mrb+\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m fp:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B20.69.237.131/home/naomi/pers_proj/cs229/code/Querying%20GPT%20%28Simulation%29.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     label_enc \u001b[39m=\u001b[39m pkl\u001b[39m.\u001b[39mload(fp)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B20.69.237.131/home/naomi/pers_proj/cs229/code/Querying%20GPT%20%28Simulation%29.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m pred_array \u001b[39m=\u001b[39m [label_enc\u001b[39m.\u001b[39mtransform([s\u001b[39m.\u001b[39mupper() \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m preds[char_id]]) \u001b[39mfor\u001b[39;00m char_id \u001b[39min\u001b[39;00m preds]\n",
      "File \u001b[0;32m/data/anaconda/envs/personality/lib/python3.9/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/label_encoders/big 5_4_all_lbls.pkl'"
     ]
    }
   ],
   "source": [
    "with open(f\"../data/label_encoders/{data_type.replace('all_lbls', 'top_lbl')}.pkl\", \"rb+\") as fp:\n",
    "    label_enc = pkl.load(fp)\n",
    "\n",
    "pred_array = [label_enc.transform([s.upper() for s in preds[char_id]]) for char_id in preds]\n",
    "label_array = [label_enc.transform(labels[char_id]) for char_id in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862 862\n"
     ]
    }
   ],
   "source": [
    "print(len(pred_array), len(label_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_means = [np.mean(char_lbls) for char_lbls in pred_array]\n",
    "label_means = [np.mean(char_lbls) for char_lbls in label_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8121660231063195\n",
      "0.4969408783160789\n"
     ]
    }
   ],
   "source": [
    "pred_sample_mean_mean = np.mean(pred_means)\n",
    "label_sample_mean_mean = np.mean(label_means)\n",
    "\n",
    "print(pred_sample_mean_mean)\n",
    "print(label_sample_mean_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3002969691467325\n",
      "0.46735324985832877\n"
     ]
    }
   ],
   "source": [
    "pred_sample_mean_std = np.std(pred_means)\n",
    "label_sample_mean_std = np.std(label_means)\n",
    "\n",
    "print(pred_sample_mean_std)\n",
    "print(label_sample_mean_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_sample_mean_std = np.std(pred_means)\n",
    "# label_sample_mean_std = np.std(label_means)\n",
    "\n",
    "# print(pred_sample_mean_std)\n",
    "# print(label_sample_mean_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_sample_std_mean = np.std(pred_means)\n",
    "# label_sample_std_mean = np.std(label_means)\n",
    "\n",
    "# print(pred_sample_mean_std)\n",
    "# print(label_sample_mean_std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
