{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from data_processing import pers_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PWD = os.environ[\"WORKSPACE_PATH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting label types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'big 5_4_top_lbl'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pers_mode = pers_labels.BIG_5\n",
    "dim_index = 4\n",
    "\n",
    "config_name = f\"{pers_mode.lower()}_{dim_index}_top_lbl\"\n",
    "\n",
    "config_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{PWD}/data/cornell_movies/speakers.json\", \"r+\") as fp:\n",
    "    fp_parsed = json.load(fp)\n",
    "    chars_meta = {}\n",
    "    chars_meta_rows = []\n",
    "    for char in fp_parsed:\n",
    "        meta = fp_parsed[char][\"meta\"]\n",
    "        meta[\"character_name\"] = meta[\"character_name\"].lower()\n",
    "        meta[\"char_id\"] = char\n",
    "        chars_meta[char] = meta\n",
    "        chars_meta_rows.append(meta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers_df = pers_labels.get_pers_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(row, mode, dim_index):\n",
    "    dim_labels = pers_labels.get_dim_labels(row, mode)\n",
    "    return dim_labels[dim_index] if dim_labels else dim_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_id_to_lbl = dict()\n",
    "for row in pers_df.iloc:\n",
    "    char_id_to_lbl[row.char_id] = get_label(row, pers_mode, dim_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## joining convos and speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/nlp/scr/neigbe/pers_proj/data/cornell_movies/utterances.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPWD\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/data/cornell_movies/utterances.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m      2\u001b[0m     utt_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(fp, lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/nlp/scr/neigbe/miniconda3/envs/personality/lib/python3.9/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/nlp/scr/neigbe/pers_proj/data/cornell_movies/utterances.jsonl'"
     ]
    }
   ],
   "source": [
    "with open(f\"{PWD}/data/cornell_movies/utterances.jsonl\", \"r+\") as fp:\n",
    "    utt_df = pd.read_json(fp, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "convos = defaultdict(lambda: {\"characters\": set(), \"lines\": []})\n",
    "\n",
    "for row in utt_df.iloc:\n",
    "    convos[row.conversation_id][\"lines\"].append({\n",
    "        \"id\": row.id,\n",
    "        \"speaker\": row.speaker,\n",
    "        \"text\": row.text\n",
    "    })\n",
    "    convos[row.conversation_id][\"movie\"] = row.meta[\"movie_id\"]\n",
    "    convos[row.conversation_id][\"characters\"].add(row.speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_convo(lines):\n",
    "    fmtd_lines = []\n",
    "    for l in sorted(lines, key=lambda l: l[\"id\"]):\n",
    "        char_id = l['speaker']\n",
    "        char = chars_meta[char_id][\"character_name\"]\n",
    "        fmtd_lines.append(f\"{char}: {l['text']}\")\n",
    "    return \"\\n\".join(fmtd_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line_count(lines, chr):\n",
    "    return len([l for l in lines if l[\"speaker\"] == chr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_scenes = defaultdict(list)\n",
    "\n",
    "for conv_id in convos:\n",
    "    conv_info = convos[conv_id]\n",
    "    lines = conv_info[\"lines\"]\n",
    "    fmtd_convo = format_convo(lines)\n",
    "\n",
    "    for char in conv_info[\"characters\"]:\n",
    "        if char in char_id_to_lbl:\n",
    "            if (lbl := char_id_to_lbl[char]) and get_line_count(lines, char) >= 3:\n",
    "                char_to_scenes[char].append(fmtd_convo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "866"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = []\n",
    "for char in char_to_scenes:\n",
    "    scenes = \"\\n\\n\".join(char_to_scenes[char])\n",
    "    text = f\"Please categorize {chars_meta[char]['character_name']}.\\n\\n{scenes}\"\n",
    "    dataset.append({\"text\": text, \"label\": char_id_to_lbl[char], \"char_id\": char, \"movie_id\": chars_meta[char][\"movie_idx\"]})\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{PWD}/data/model_datasets/{config_name}.jsonl\", \"w+\") as fp:\n",
    "    fp.write(\"\\n\".join([json.dumps(ex) for ex in dataset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
